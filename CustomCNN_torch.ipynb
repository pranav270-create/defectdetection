{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CustomCNN_torch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"],"metadata":{"id":"T0IOvOVSS_5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZPOUYtf5LYO"},"outputs":[],"source":["# importing required libraries\n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","from skimage import color\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["def rescale(i, image, width, height):\n","  new_im = cv2.resize(image,(width,height))\n","  # normalize values to be between -1 and 1\n","  # new_im = (new_im - 127.5 )/127.5\n","  # normalize values to be between 0 and 1\n","  # new_im = new_im/255\n","  return new_im"],"metadata":{"id":"rI2N2Gk9N0e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We first load the necessary libraries, the dataset and reshape its dimensons\n","\n","# FOR INITIAL RAW IMAGES\n","faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/def_front/'\n","ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/ok_front/'\n","\n","# FOR AUGMENTED IMAGES\n","#faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/def_front/'\n","#ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/ok_front/'\n","directories = (faulty_dir, ok_dir)\n","\n","# check the total number of images we have\n","count = 0\n","for direct in directories:\n","  for file in os.listdir(direct):\n","    count += 1\n","\n","# set the values of the X and y arrays\n","num_images = count\n","input_shape = (80, 80, 3)\n","height = input_shape[0]\n","width = input_shape[1]\n","channels = input_shape[2]\n","\n","X = np.zeros((num_images,height,width,channels))\n","y = np.zeros((num_images))\n","\n","# populate the arrays\n","i = 0\n","j = -1\n","for direct in directories:\n","  j+=1\n","  for file in os.listdir(direct):\n","    path = (direct+file)\n","    im = plt.imread(path)\n","    scaled_im = rescale(i, im, width, height)\n","    X[i,:,:,:] = scaled_im\n","    if j == 0:\n","      # 1 for faulty\n","      y[i] = 1\n","    else:\n","      # 0 for ok image\n","      y[i] = 0\n","    i+=1\n","\n","# one-hot encoding using function from Keras\n","# y = to_categorical(y)"],"metadata":{"id":"885aZPsT6Owb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use Keras function to split the arrays into training, test, and validation (70%, 15%, 15%)\n","X_train, X_valid_test, y_train, y_valid_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size = 0.5, random_state=1)\n","\n","print(X_train.shape, X_valid.shape, X_test.shape)\n","print(type(X_train),type(y_train))"],"metadata":{"id":"RrCNHS98CAy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f, axarr = plt.subplots(3,3,figsize=(5,5))\n","for i in range(9):\n","  row = int(i/3)\n","  col = int(i%3)\n","  axarr[row,col].imshow(X[i,:,:,0], cmap='gray')\n","plt.show()"],"metadata":{"id":"3R4yCLJzGhxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.autograd import Variable\n","from Networks_Implementation import Classifier_CNN\n","model = Classifier_CNN()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n","loss_fn = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"l-wGvuqz_B-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  BATCH_SIZE = 128\n","  X_train = X_train.transpose(X_train,(2,0,1))\n","  X_train = torch.Tensor(X_train)\n","  y_train = torch.Tensor(y_train).type(torch.FloatTensor)\n","  dataset_train = TensorDataset(X_train,y_train)\n","\n","  X_valid = X_valid.transpose(X_valid,(2,0,1))\n","  X_valid = torch.Tensor(X_valid)\n","  y_valid = torch.Tensor(y_valid).type(torch.FloatTensor)\n","  dataset_valid = TensorDataset(X_valid,y_valid)\n","\n","  train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n","  valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"Nafs8ugLcQYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def saveModel():\n","    path = \"/content/drive/Shareddrives/Senior Thesis/Models/Classifier.pth\"\n","    torch.save(model.state_dict(), path)\n","\n","def testAccuracy(dataloader):\n","    model.eval()\n","    accuracy = 0.0\n","    total = 0.0\n","    \n","    with torch.no_grad():\n","        for data in dataloader:\n","            images, labels = data\n","            # run the model on the test set to predict labels\n","            outputs = model(images)\n","            # the label with the highest probability will be our prediction\n","            predicted = torch.squeeze(torch.round(outputs))\n","            total += labels.shape[0]\n","            accuracy += (predicted == labels).sum().item()\n","    \n","    # compute the accuracy over all test images\n","    accuracy = (100 * accuracy / total)\n","    return(accuracy) \n","\n","def train(num_epochs):\n","    \n","    best_accuracy = 0.0\n","    model.train()\n","\n","    # Define your execution device\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"The model will be running on\", device, \"device\")\n","    # Convert model parameters and buffers to CPU or Cuda\n","    model.to(device)\n","\n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","        running_loss = 0.0\n","        running_acc = 0.0\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            \n","            # get the inputs\n","            images = Variable(images.to(device))\n","            labels = Variable(labels.to(device))\n","            labels = labels.unsqueeze(1)\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","            # predict classes using images from the training set\n","            outputs = model(images)\n","            # compute the loss based on model output and real labels\n","            loss = loss_fn(outputs, labels)\n","            # backpropagate the loss\n","            loss.backward()\n","            # adjust parameters based on the calculated gradients\n","            optimizer.step()\n","\n","            # Let's print statistics for every 1,000 images\n","            running_loss += loss.item()     # extract the loss value\n","            if i % 10 == 0:    \n","                # print every 10\n","                print('[%d, %5d] loss: %.3f' %\n","                      (epoch + 1, i + 1, running_loss / 1000))\n","                # zero the loss\n","                running_loss = 0.0\n","\n","        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n","        accuracy = testAccuracy(valid_loader)\n","        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n","        \n","        # we want to save the model if the accuracy is the best\n","        if accuracy > best_accuracy:\n","            saveModel()\n","            best_accuracy = accuracy"],"metadata":{"id":"lVI8x2bej2_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(20)"],"metadata":{"id":"XjNg4isho9pZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot training and validation losses and accuracy versus number of epochs\n","accuracy = model.history.history['accuracy']\n","val_accuracy = model.history.history['val_accuracy']\n","loss = model.history.history['loss']\n","val_loss = model.history.history['val_loss']\n","epochs = range(len(accuracy))\n","plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n","plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.xlim(0,10)\n","plt.ylim(0,1.0)\n","plt.figure()\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"q8GEmIQwDAnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = np.zeros((2,2))\n","\n","metrics[0,0] = model.history.history['true_positives'][9]\n","metrics[0,1] = model.history.history['false_positives'][9]\n","metrics[1,0] = model.history.history['false_negatives'][9]\n","metrics[1,1] = model.history.history['true_negatives'][9]\n","print(metrics)"],"metadata":{"id":"623a2am2IlZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval(x = X_test, y = y_test)"],"metadata":{"id":"qO54a0LUrFts"},"execution_count":null,"outputs":[]}]}