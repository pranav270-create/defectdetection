{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PretrainedClassifier.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "T0IOvOVSS_5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZPOUYtf5LYO"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras import callbacks\n",
        "from keras import optimizers\n",
        "#from keras.engine import Model\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We first load the necessary libraries, the dataset and reshape its dimensons\n",
        "\n",
        "# FOR INITIAL RAW IMAGES\n",
        "#faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/def_front/'\n",
        "#ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/ok_front/'\n",
        "\n",
        "# FOR AUGMENTED IMAGES\n",
        "faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/def_front/'\n",
        "ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/ok_front/'\n",
        "directories = (faulty_dir, ok_dir)\n",
        "\n",
        "\n",
        "def rescale(i, image, width, height):\n",
        "  new_im = cv2.resize(image,(width,height))\n",
        "  # normalize values to be between 0 and 1\n",
        "  new_im = new_im / 255\n",
        "  return new_im\n",
        "\n",
        "# check the total number of images we have\n",
        "count = 0\n",
        "for direct in directories:\n",
        "  for file in os.listdir(direct):\n",
        "    count += 1\n",
        "\n",
        "# set the values of the X and y arrays\n",
        "num_images = count\n",
        "input_shape = (300, 300, 3)\n",
        "height = input_shape[0]\n",
        "width = input_shape[1]\n",
        "channels = input_shape[2]\n",
        "\n",
        "X = np.zeros((num_images,height,width,channels))\n",
        "y = np.zeros((num_images))\n",
        "\n",
        "# populate the arrays\n",
        "i = 0\n",
        "j = -1\n",
        "for direct in directories:\n",
        "  j+=1\n",
        "  for file in os.listdir(direct):\n",
        "    path = (direct+file)\n",
        "    im = plt.imread(path)\n",
        "    scaled_im = rescale(i, im, width, height)\n",
        "    X[i,:,:,:] = scaled_im\n",
        "    if j == 0:\n",
        "      # 1 for faulty\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      # 0 for ok image\n",
        "      y[i] = 0\n",
        "    i+=1\n",
        "\n",
        "# one-hot encoding using function from Keras\n",
        "y = to_categorical(y)\n",
        "\n",
        "# use Keras function to split the arrays into training, test, and validation (70%, 15%, 15%)\n",
        "X_train, X_valid_test, y_train, y_valid_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size = 0.5, random_state=1)\n"
      ],
      "metadata": {
        "id": "885aZPsT6Owb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_valid_test.shape, X_valid.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "RrCNHS98CAy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We build the base model\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "#base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "from keras.applications import EfficientNetB3\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=input_shape, drop_connect_rate=0.2)"
      ],
      "metadata": {
        "id": "HEBy2oM48eex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We freeze every layer in our base model so that they do not train, we want that our feature extractor stays as before --> transfer learning\n",
        "for layer in base_model.layers: \n",
        "  layer.trainable = False\n",
        "  print('Layer ' + layer.name + ' frozen.')\n",
        "# We take the last layer of our the model and add it to our classifier\n",
        "last = base_model.layers[-1].output\n",
        "x = Flatten()(last)\n",
        "x = Dense(1000, activation='relu', name='fc1')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation='softmax', name='predictions')(x)\n",
        "model = tf.keras.Model(base_model.input, x)\n",
        "\n",
        "# We compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "l-wGvuqz_B-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We start the training\n",
        "mcp_save = ModelCheckpoint('weights.hdf5', save_best_only=True, monitor='val_acc', mode='auto')\n",
        "model.fit(x=X_train, y=y_train, batch_size=128, epochs=10, verbose=1, callbacks=[mcp_save], validation_data=(X_valid,y_valid), shuffle=True)"
      ],
      "metadata": {
        "id": "nmxVv6HnAlOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and validation losses and accuracy versus number of epochs\n",
        "accuracy = model.history.history['accuracy']\n",
        "val_accuracy = model.history.history['val_accuracy']\n",
        "loss = model.history.history['loss']\n",
        "val_loss = model.history.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8GEmIQwDAnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unfreeze_model(model):\n",
        "    # We unfreeze the top 10 layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-10:]:\n",
        "        if not isinstance(layer, model.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "7rAlbDLIR73y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unfreeze_model(model)\n",
        "\n",
        "# We now fine-tune the training\n",
        "mcp_save = ModelCheckpoint('weights.hdf5', save_best_only=True, monitor='val_acc', mode='auto')\n",
        "model.fit(x=X_train, y=y_train, batch_size=128, epochs=epochs, verbose=1, callbacks=[mcp_save], validation_data=(X_valid,y_valid), shuffle=True)"
      ],
      "metadata": {
        "id": "Q93puxRNSkrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and validation losses and accuracy versus number of epochs\n",
        "accuracy = model.history.history['accuracy']\n",
        "val_accuracy = model.history.history['val_accuracy']\n",
        "loss = model.history.history['loss']\n",
        "val_loss = model.history.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RC2J2lgKSwRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}