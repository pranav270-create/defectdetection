{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PretrainedClassifier.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow_addons"],"metadata":{"id":"qz0kdn1jMDt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZPOUYtf5LYO"},"outputs":[],"source":["# importing required libraries\n","import tensorflow as tf \n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","from skimage import color\n","import scipy\n","import math\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","import keras\n","from keras import callbacks\n","from keras import optimizers\n","#from keras.engine import Model\n","from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization\n","from keras.utils.np_utils import to_categorical\n","from keras.models import Sequential\n","from keras.callbacks import ModelCheckpoint\n","\n","from keras.preprocessing import image\n","from keras.applications.vgg16 import preprocess_input\n","from keras.applications.vgg16 import decode_predictions\n","\n","from google.colab import drive\n","from google.colab.patches import cv2_imshow\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["def rescale(i, image, width, height):\n","  new_im = cv2.resize(image,(width,height))\n","  # normalize values to be between -1 and 1\n","  # new_im = (new_im - 127.5 )/127.5\n","  # normalize values to be between 0 and 1\n","  # new_im = new_im/255\n","  return new_im"],"metadata":{"id":"rI2N2Gk9N0e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We first load the necessary libraries, the dataset and reshape its dimensons\n","\n","# FOR INITIAL RAW IMAGES\n","faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/def_front/'\n","ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_512x512/casting_512x512/ok_front/'\n","\n","# FOR AUGMENTED IMAGES\n","#faulty_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/def_front/'\n","#ok_dir = '/content/drive/Shareddrives/Senior Thesis/Casting_image/casting_data/casting_data/images/ok_front/'\n","directories = (faulty_dir, ok_dir)\n","\n","# check the total number of images we have\n","count = 0\n","for direct in directories:\n","  for file in os.listdir(direct):\n","    count += 1\n","\n","# set the values of the X and y arrays\n","num_images = count\n","dim = 280\n","input_shape = (dim, dim, 3)\n","height = input_shape[0]\n","width = input_shape[1]\n","channels = input_shape[2]\n","\n","X = np.zeros((num_images,height,width,channels))\n","y = np.zeros((num_images))\n","\n","# populate the arrays\n","i = 0\n","j = -1\n","for direct in directories:\n","  j+=1\n","  for file in os.listdir(direct):\n","    path = (direct+file)\n","    im = plt.imread(path)\n","    scaled_im = rescale(i, im, width, height)\n","    #edges, gradient = filter(scaled_im[:,:,0])\n","    X[i] = scaled_im\n","    #X[i,:,:,1] = edges/255\n","    #X[i,:,:,2] = (gradient-np.min(gradient))/(np.max(gradient)-np.min(gradient))\n","    if j == 0:\n","      # 1 for faulty\n","      y[i] = 1\n","    else:\n","      # 0 for ok image\n","      y[i] = 0\n","    i+=1"],"metadata":{"id":"885aZPsT6Owb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert = False\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import AgglomerativeClustering\n","\n","if convert:\n","  dims = 50\n","  X_train = X\n","  y_train = y\n","  X_train_fit = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3])\n","  pca = PCA(n_components = dims)\n","  X_train_fit = pca.fit_transform(X_train_fit)\n","  #kmeans clustering on PCA data\n","  data = AgglomerativeClustering(n_clusters = 3, compute_distances=True).fit(X_train_fit)\n","\n","  idx_train0 = np.array(np.where(y_train==0)).flatten()\n","  idx_train1 = np.array(np.where(y_train==1)).flatten()\n","  idx_labels0 = np.array(np.where(data.labels_==0)).flatten()\n","  idx_labels1 = np.array(np.where(data.labels_==1)).flatten()\n","  idx_labels2 = np.array(np.where(data.labels_==2)).flatten()\n","\n","  y[np.intersect1d(idx_train1, idx_labels0)] = 1 #defective type 1\n","  y[np.intersect1d(idx_train1, idx_labels2)] = 2 #defective type 2\n","  \n","  print(np.array(np.where(y==0)).flatten().shape)\n","  print(np.array(np.where(y==1)).flatten().shape)\n","  print(np.array(np.where(y==2)).flatten().shape)"],"metadata":{"id":"Vr6z_qQrIVUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use Keras function to split the arrays into training, test, and validation (70%, 15%, 15%)\n","y_cat = to_categorical(y)\n","desired_size = 500\n","frac = 1-desired_size/len(X)\n","#X_train, y_train = X,y_cat\n","X_train, X_valid_test, y_train, y_valid_test = train_test_split(X, y_cat, test_size = frac, random_state=1)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size = 0.5, random_state=1)\n","\n","print(X_train.shape, X_valid.shape, X_test.shape)\n","print(y_train.shape)\n","print(np.min(X_train[:,:,:,0]),np.max(X_train[:,:,:,0]))"],"metadata":{"id":"RrCNHS98CAy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_method(X, y, p):\n","  X_train_aug = []\n","  y_train_aug = []\n","  for i in range(len(X)):\n","    im = X[i]\n","    z = np.random.binomial(1,p)\n","    if z==1:\n","      x = np.random.uniform(0,1,1)\n","      if x<0.25: #shear\n","        val_x = float(np.random.uniform(0,0.3,1))\n","        val_y = float(np.random.uniform(0,0.3,1))\n","        transf = ((1, val_x, 0, val_y, 1, 0, 0, 0))\n","        im = tfa.image.transform(im, transf, fill_value = 150)\n","        #cv2_imshow(np.array(im))\n","      elif x>0.25 and x<0.5: #rotation\n","        angle = np.random.uniform(0,2*math.pi,1)\n","        im = tfa.image.rotate(im, angle, fill_value = 150)\n","        #cv2_imshow(np.array(im))\n","      elif x>0.5 and x<0.75: #crop and resize\n","        box = np.random.uniform(0.6,0.9,1)\n","        im = im[0:int(box*dim), 0:int(box*dim),:]\n","        im = tf.image.resize(im, (dim,dim))\n","        #cv2_imshow(np.array(im))\n","      else: #brightening\n","        delta = np.random.uniform(-0.7,0.7,1)\n","        im = tf.image.adjust_brightness(im, delta)\n","        #cv2_imshow(np.array(im))\n","      X_train_aug.append(im)\n","      y_train_aug.append(y[i])\n","  return np.array(X_train_aug), np.array(y_train_aug)"],"metadata":{"id":"3xJFKIkCJR4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_mix(X, y, same_class, size, frac):\n","  if same_class:\n","    num_defective = np.count_nonzero(y == 1)\n","    num_ok = len(X) - num_defective\n","    tot = num_defective**2 + num_ok**2\n","    assert size < tot\n","    X_train_mix = []\n","    y_train_mix= []\n","    indices_defect = np.array(np.where(y==1)).flatten() #defective class mixing\n","    indices_normal = np.array(np.where(y==0)).flatten() #normal class mixing\n","    print(num_defective, num_ok)\n","    for i in range(num_defective):\n","      for j in range(num_defective):\n","        if i!=j and len(X_train_mix) < int(size*0.5):\n","            im = (X[indices_defect[i],:,:,:] + X[indices_defect[j],:,:,:])/2\n","            X_train_mix.append(im)\n","            y_train_mix.append(y[indices_defect[i]])\n","    for i in range(num_ok):\n","      for j in range(num_ok):\n","        if i!=j and len(X_train_mix) < size:\n","            im = (X[indices_normal[i],:,:,:] + X[indices_normal[j],:,:,:])/2\n","            X_train_mix.append(im)\n","            y_train_mix.append(y[indices_normal[i]])\n","  else:\n","    assert size < len(X)**2\n","    X_train_mix = []\n","    y_train_mix= []\n","    for i in range(len(X)):\n","      for j in range(len(X)):\n","        if i!=j and len(X_train_mix) < size:\n","            if frac:\n","              alpha = np.random.uniform(0.3,0.7,1)\n","            else:\n","              alpha = 0.5\n","            beta = 1-alpha\n","            im = (alpha*X[i,:,:,:] + beta*X[j,:,:,:])\n","            X_train_mix.append(im)\n","            if alpha>=0.5: #arbitrary choice here with =0.5\n","              y_train_mix.append(y[i]) \n","            else:\n","              y_train_mix.append(y[j])\n","  return np.array(X_train_mix), np.array(y_train_mix)"],"metadata":{"id":"eYFV0VREpGK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def random_del(X, y):\n","  X_train_del = []\n","  y_train_del= []\n","  height = X[0,:,:,:].shape[0]\n","  width = X[0,:,:,:].shape[1]\n","  for i in range(len(X)):\n","    coord1 = int(np.random.uniform(0,height,1))\n","    coord2 = int(np.random.uniform(0,width,1))\n","    len1 = int(np.random.uniform(coord1,height-coord1,1))\n","    len2 = int(np.random.uniform(coord2,width-coord2,1))\n","    im = X[i,:,:,:]\n","    im[coord1:coord1+len1,coord2:coord2+len2,:] = 0\n","    X_train_del.append(im)\n","    y_train_del.append(y[i])\n","  return np.array(X_train_del), np.array(y_train_del)"],"metadata":{"id":"zMMkKImexiSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def zca_whitening(X):\n","    #X_new = np.zeros((X.shape[0], X.shape[1], X.shape[2], X.shape[3]))\n","    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n","    for i in range(X.shape[0]):\n","      X_norm = X[i]/255\n","      X[i] = X_norm - X_norm.mean()\n","    # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) / N\n","    sigma = np.cov(X, rowvar=False) # [M x M]\n","    print(sigma.shape)\n","    # Singular Value Decomposition. X = U * np.diag(S) * V\n","    U,S,V = scipy.linalg.svd(sigma, full_matrices=True)\n","        # U: [M x M] eigenvectors of sigma.\n","        # S: [M x 1] eigenvalues of sigma.\n","        # V: [M x M] transpose of U\n","    # Whitening constant: prevents division by zero\n","    epsilon = 0.1\n","    # ZCA Whitening matrix: U * Lambda * U'\n","    X_ZCA = U.dot(np.diag(1.0/np.sqrt(S + epsilon))).dot(U.T).dot(X_norm.T).T # [M x M]\n","    X_ZCA_rescaled = ((X_ZCA - X_ZCA.min()) / (X_ZCA.max() - X_ZCA.min()))\n","    X_ZCA_rescaled = X_ZCA_rescaled.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n","    #X_new[i,:,:,:] = X_ZCA_rescaled\n","    return X_ZCA_rescaled"],"metadata":{"id":"OIT1lMMehNQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ZCA = tf.keras.preprocessing.image.ImageDataGenerator(zca_whitening=True,zca_epsilon=1e-06)\n","# ZCA.fit(X_train)"],"metadata":{"id":"9Cz6FQUODLzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from contextlib import nullcontext\n","#Use k-means clustering\n","def run_kmeans(K, niter, image):\n","    \n","    # Load and transform an image\n","    img = image.astype('uint8')\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    height, width, nchannel = img.shape\n","    #scale = 500/max(height, width) # The longer side will be resized to 500\n","    #img = cv2.resize(img, (int(width*scale), int(height*scale)))\n","\n","    # Vectorize the image\n","    x = img.reshape((-1, 3)).astype(np.float32)\n","    \n","    # Run K-means clustering\n","    labels, centroids = kmeans(x, K, niter)\n","    \n","    if labels is not None:\n","      # Produce the resulting image segmentation. \n","      centroids = np.uint8(centroids)\n","      labels = np.int8(labels)\n","      result = centroids[labels.flatten()]\n","      result_image = result.reshape((img.shape))\n","      return result_image\n","    else:\n","      return None\n","    # Visualize the original image and the segmentation.\n","    # plt.figure(figsize=(5, 5))\n","    # plt.subplot(1,2,1)\n","    # plt.imshow(img); plt.title('Original Image'); plt.axis('off')\n","    # plt.subplot(1,2,2)\n","    # plt.imshow(result_image); plt.title('Segmented Image (K={})'.format(K)); plt.axis('off')\n","    # plt.show()\n","\n","def kmeans(x, K, niter, seed=123):\n","\n","    np.random.seed(seed)\n","    idx = np.random.choice(len(x), K, replace=False)\n","\n","    # Randomly choose centroids\n","    centroids = x[idx, :]\n","\n","    # Initialize labels\n","    labels = np.zeros((x.shape[0], ))\n","    changed = True\n","    i = 0\n","    \n","    while changed:\n","        \n","        if i == niter - 1:\n","            changed = False\n","            \n","        X_norms = np.array([np.sum(np.square(x),axis=1)])\n","        # Square of the matrix elements and summing across the rows\n","        centroids_norms = np.array([np.sum(np.square(centroids),axis=1)])\n","        # Finding xi*wj by doing X*WT\n","        X_dot_centroids = 2*np.matmul(x,np.transpose(centroids))\n","\n","        distances = np.transpose(X_norms) + centroids_norms - X_dot_centroids\n","        temp = np.argmin(distances, axis = 1)\n","        \n","        if np.array_equal(temp, labels):\n","            changed = False \n","        \n","        labels = temp\n","        centroid_labels, centroid_counts = np.unique(labels, return_counts=True)\n","        indx = np.isin(np.arange(K), centroid_labels)\n","\n","        indx = indx.flatten()\n","        # number of times each centroid is associated with a point\n","        counts = np.zeros(K)\n","        # The index of the count matrix corresponds to the number of times the entry appears in u2\n","        #print(indx.shape, counts.shape, centroid_counts.shape)\n","        if (centroid_counts.shape!=counts.shape):\n","          return None, None\n"," \n","        counts[indx] = centroid_counts[indx]\n","        # adding 1e-20 to avoid division by 0 in final line\n","        counts = counts.reshape(K, 1) + 1e-20 \n","        # vectorized way of calculating sum over rows associated with each cluster\n","        sums = (np.eye(K)[labels]).T @ x\n","        centroids = sums/counts\n","        \n","        i+=1\n","    \n","    return labels, centroids"],"metadata":{"id":"EnGQJp9iBq3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sep = False\n","\n","same_class = True\n","size = 2*len(X_train)\n","frac = False\n","\n","mix = False\n","if mix:\n","  if sep:\n","    X_mix, y_mix = sample_mix(X_train,y_train, same_class, size, frac)\n","    print(X_train.shape, y_train.shape)\n","    print(X_mix.shape, y_mix.shape)\n","  else:\n","    X_mix, y_mix = sample_mix(X_train,y_train, same_class, size, frac)\n","    print(X_train.shape, y_train.shape)\n","    print(X_mix.shape, y_mix.shape)\n","    X_train = np.concatenate((X_train,X_mix))\n","    y_train = np.concatenate((y_train,y_mix))\n","    print(X_train.shape, y_train.shape)\n","\n","augment = False\n","if augment:\n","  if sep:\n","    X_augment, y_augment = augment_method(X_train,y_train,1)\n","    print(X_train.shape, y_train.shape)\n","    print(X_augment.shape, y_augment.shape)\n","  else:\n","    X_augment, y_augment = augment_method(X_train,y_train,1)\n","    print(X_train.shape, y_train.shape)\n","    print(X_augment.shape, y_augment.shape)\n","    X_train = np.concatenate((X_train,X_augment))\n","    y_train = np.concatenate((y_train,y_augment))\n","    print(X_train.shape, y_train.shape)\n","\n","rand_del = False\n","if rand_del:\n","  if sep:\n","    X_del, y_del = random_del(X_train,y_train)\n","    print(X_train.shape, y_train.shape)\n","    print(X_del.shape, y_del.shape)\n","  else:\n","    X_del, y_del = random_del(X_train,y_train)\n","    print(X_train.shape, y_train.shape)\n","    print(X_del.shape, y_del.shape)\n","    X_train = np.concatenate((X_train,X_del))\n","    y_train = np.concatenate((y_train,y_del))\n","    print(X_train.shape, y_train.shape)\n","\n","ZCA = False\n","if ZCA:\n","  X_train = zca_whitening(X_train[0:10,:,:,:])\n","\n","GAN = False\n","X_GAN = np.zeros_like(X_train)\n","y_GAN = np.zeros(X_train.shape[0])\n","if GAN:\n","  GAN_photos = '/content/drive/Shareddrives/Senior Thesis/GAN_Images/'\n","  i = 0\n","  for file in os.listdir(GAN_photos):\n","    while i < len(X_train):\n","      path = (GAN_photos+file)\n","      im = plt.imread(path)\n","      scaled_im = rescale(i, im, width, height)\n","      scaled_im = scaled_im[:,:,np.newaxis]\n","      scaled_im = np.repeat(scaled_im, 3, axis = 2)\n","      X_GAN[i] = scaled_im\n","      y_GAN[i] = 1\n","      i += 1\n","  if sep:\n","    print(X_train.shape, y_train.shape)\n","    print(X_GAN.shape, y_GAN.shape)\n","  else:\n","    print(X_train.shape, y_train.shape)\n","    print(X_GAN.shape, y_GAN.shape)\n","    X_train = np.concatenate((X_train,X_GAN))\n","    y_train = np.concatenate((y_train,y_GAN))\n","    print(X_train.shape, y_train.shape)\n","\n","kmeans_do = False\n","if kmeans_do:\n","  X_kmeans = []\n","  y_kmeans = []\n","  for i in range(len(X_train)):\n","    #print(X_train[i,:,:,:].shape)\n","    #print(i)\n","    im = run_kmeans(2,10,X_train[i,:,:,:])\n","    #cv2_imshow(im)\n","    if im is not None:\n","      X_kmeans.append(im)\n","      y_kmeans.append(y[i])\n","  X_kmeans = np.array(X_kmeans)\n","  y_kmeans = np.array(y_kmeans)\n","\n","from sklearn.decomposition import PCA\n","dims = 50\n","PCA_go = False\n","if PCA_go:\n","  X_train_fit = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3])\n","  pca = PCA(n_components=dims)\n","  X_train_fit = pca.fit_transform(X_train_fit)\n","  #X_train_fit = X_train_fit.reshape(X_train.shape[0], X_train.shape[1],X_train.shape[2],X_train.shape[3])\n","\n","  # X_valid_fit = X_valid.reshape(X_valid.shape[0], X_valid.shape[1]*X_valid.shape[2]*X_valid.shape[3])\n","  # pca = PCA(n_components=dims)\n","  # X_valid_fit = pca.fit_transform(X_valid_fit)\n","  #X_valid_fit = X_valid_fit.reshape(X_valid.shape[0], X_valid.shape[1],X_valid.shape[2],X_valid.shape[3])"],"metadata":{"id":"52HjyvyCSFu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(X_kmeans.shape)\n","# desired_size = 100\n","# frac = 1-desired_size/len(X)\n","# X_train, y_train = X,y\n","# X_train, X_valid_test, y_train, y_valid_test = train_test_split(X_kmeans, y_kmeans, test_size = frac, random_state=1)\n","# X_valid, X_test, y_valid, y_test = train_test_split(X_valid_test, y_valid_test, test_size = 0.5, random_state=1)\n","\n","# print(X_train.shape, X_valid.shape, X_test.shape)"],"metadata":{"id":"2VpwcYTwGDf9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f, axarr = plt.subplots(2,2,figsize=(3,3))\n","for i in range(4):\n","  row = int(i/2)\n","  col = int(i%2)\n","  axarr[row,col].imshow(X[i+10,:,:,0], cmap='gray')\n","plt.show()"],"metadata":{"id":"3R4yCLJzGhxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We build the base model\n","from keras.applications.vgg16 import VGG16\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n","#X_train = tf.keras.applications.vgg16.preprocess_input(X_train)\n","#X_valid = tf.keras.applications.vgg16.preprocess_input(X_valid)"],"metadata":{"id":"HEBy2oM48eex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras.backend as K\n","def custom_loss(y_true, y_pred):       \n","\n","    loss = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n","    FN = K.mean(K.greater(y_true, y_pred)) #Total False Negatives / Total Batch Size\n","    # FN = np.logical_and(y_true == 1, y_pred == 0)\n","    loss = loss + 3*FN\n","    return loss"],"metadata":{"id":"tR0GLk2HEdHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We freeze every layer in our base model so that they do not train, we want that our feature extractor stays as before --> transfer learning\n","for layer in base_model.layers: \n","  layer.trainable = False\n","  print('Layer ' + layer.name + ' frozen.')\n","# We take the last layer of our the model and add it to our classifier\n","last = base_model.layers[-1].output\n","x = Flatten()(last)\n","x = Dense(1000, activation='relu', name='fc1')(x)\n","x = Dropout(0.5)(x)\n","x = Dense(2, activation='softmax', name='predictions')(x)\n","model = tf.keras.Model(base_model.input, x)\n","\n","# We compile the model\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","loss_entropy = tf.keras.losses.CategoricalCrossentropy()\n","loss_binary_entropy = tf.keras.losses.BinaryCrossentropy()\n","model.compile(optimizer=optimizer, loss=loss_entropy, metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n","#model.summary()\n","from keras.utils.vis_utils import plot_model\n","#plot_model(model, to_file='classifier_plot.png', show_shapes=True, show_layer_names=True)"],"metadata":{"id":"l-wGvuqz_B-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We start the training\n","#tf.config.experimental_run_functions_eagerly(True)\n","config = 1\n","transform = \"forSegmentation\"\n","preprocess = \"\"\n","imsize = str(dim)\n","path_name = f\"{'/content/drive/Shareddrives/Senior Thesis/Models/my_model_'}{config}{'_'}{preprocess}{'_'}{transform}{'_'}{imsize}{'.hdf5'}\"\n","earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n","mcp_save = tf.keras.callbacks.ModelCheckpoint(path_name, save_best_only=True, monitor='val_loss', mode='min')\n","reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n","sep = False\n","if sep:\n","  for i in range(5):\n","    model.fit(x=X_mix, y=y_mix, batch_size=128, epochs=8, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=(X_valid,y_valid), shuffle=True)\n","    model.fit(x=X_train, y=y_train, batch_size=128, epochs=2, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=(X_valid,y_valid), shuffle=True)\n","else:\n","  model.fit(x=X_train, y=y_train, batch_size=128, epochs=50, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=(X_valid,y_valid), shuffle=True)"],"metadata":{"id":"nmxVv6HnAlOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(path_name)"],"metadata":{"id":"J9nUql8eoohk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot = False\n","accuracy = model.history.history['accuracy']\n","val_accuracy = model.history.history['val_accuracy']\n","loss = model.history.history['loss']\n","val_loss = model.history.history['val_loss']\n","precision = model.history.history['precision']\n","recall = model.history.history['recall']\n","if plot:\n","  epochs = range(len(accuracy))\n","  plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n","  plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.ylim(0,1.0)\n","  plt.figure()\n","  plt.plot(epochs, loss, 'b', label='Training loss')\n","  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"q8GEmIQwDAnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def unfreeze_model(model):\n","    # We unfreeze the top 10 layers while leaving BatchNorm layers frozen\n","    for layer in model.layers[-5:]:\n","        #if type(layer) == model.layers.BatchNormalization:\n","        layer.trainable = True\n","\n","    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","    #model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"],"metadata":{"id":"7rAlbDLIR73y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unfreeze = False\n","if unfreeze:\n","  unfreeze_model(model)\n","  epochs = 20\n","  batch_size = 128\n","\n","  # We now fine-tune the training\n","  mcp_save = tf.keras.callbacks.ModelCheckpoint(path_name, save_best_only=True, monitor='val_loss', mode='min')\n","  model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=(X_valid,y_valid), shuffle=True)\n","  \n","  # plot training and validation losses and accuracy versus number of epochs\n","  accuracy_full = model.history.history['accuracy']\n","  val_accuracy_full = model.history.history['val_accuracy']\n","  loss_full = model.history.history['loss']\n","  val_loss_full = model.history.history['val_loss']\n","  precision_full = model.history.history['precision']\n","  recall_full = model.history.history['recall']\n","\n","\n","  accuracy_tot = np.concatenate((np.array(accuracy),np.array(accuracy_full)))\n","  val_accuracy_tot = np.concatenate((np.array(val_accuracy),np.array(val_accuracy_full)))\n","  loss_tot = np.concatenate((np.array(loss),np.array(loss_full)))\n","  val_loss_tot = np.concatenate((np.array(val_loss),np.array(val_loss_full)))\n","  precision_tot = np.concatenate((np.array(precision),np.array(precision_full)))\n","  recall_tot = np.concatenate((np.array(recall),np.array(recall_full)))"],"metadata":{"id":"Q93puxRNSkrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#classifier = keras.models.load_model('/content/drive/Shareddrives/Senior Thesis/Models/my_model_2___100.hdf5')\n","# y_val=model.predict(X_valid)\n","# from sklearn.metrics import roc_curve,roc_auc_score, det_curve\n","# fpr_a , tpr , thresholds_a = roc_curve(y_valid , y_val, pos_label = 1)\n","# fpr_b , fnr , thresholds_b = det_curve(y_valid , y_val, pos_label = 1)"],"metadata":{"id":"qO54a0LUrFts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","if unfreeze:\n","  N = len(accuracy_tot)\n","  K = max(N, len(fpr_b))\n","  data = np.zeros((K,10))\n","  data[:N,0] = accuracy_tot\n","  data[:N,1] = val_accuracy_tot\n","  data[:N,2] = loss_tot\n","  data[:N,3] = val_loss_tot\n","  data[:N,4] = precision_tot\n","  data[:N,5] = recall_tot\n","  data[:len(fpr_a),6] = fpr_a\n","  data[:len(tpr),7] = tpr\n","  data[:len(thresholds_a),8] = thresholds_a\n","  data[:len(fpr_b),9] = fpr_b\n","  data[:len(fnr),10] = fnr\n","  data[:len(thresholds_b),11] = thresholds_b\n","else:\n","  N = len(accuracy)\n","  # K = max(N, len(fpr_b))\n","  data = np.zeros((N,12))\n","  data[:N,0] = accuracy\n","  data[:N,1] = val_accuracy\n","  data[:N,2] = loss\n","  data[:N,3] = val_loss\n","  data[:N,4] = precision\n","  data[:N,5] = recall\n","  # data[:len(fpr_a),6] = fpr_a\n","  # data[:len(tpr),7] = tpr\n","  # data[:len(thresholds_a),8] = thresholds_a\n","  # data[:len(fpr_b),9] = fpr_b\n","  # data[:len(fnr),10] = fnr\n","  # data[:len(thresholds_b),11] = thresholds_b\n","\n","df = pd.DataFrame(data)\n","df.to_excel(excel_writer = \"/content/data.xlsx\")"],"metadata":{"id":"t_D6a1bSe785"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","start = time.time()\n","model.predict(X)\n","print((time.time()-start)/len(X))"],"metadata":{"id":"LBWIjftXWwjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pjxSZoxzpPXA"},"execution_count":null,"outputs":[]}]}